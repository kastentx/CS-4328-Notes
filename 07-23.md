* midterm on friday

## FOR HW
* within j loop (j > 0)
  * close(fd[0])
  * close(fd[1])
  * l_fd[0]=f_fd[0];
  * l_fd[1]=r_fd[1];
* "parent saves the right pipe to the left pipe"
* "next child uses that pipe as its left pipe"

## Review Questions
### Ch 1
* **1.7**
  * good idea to "waste" system resources 
    * graphical user interface vs. command line... better cuz its more user friendly
* **1.8**
  * time-sharing could be better if there were a very small amount of users
* **1.9**
  * asymmetric: master/slave hierarchy structure
  * symmetric: all CPUs are the same type hierarchy
  * parallel: many CPUs, with shared memory and clock (tightly coupled)
  * distributed: many CPUs, each with their own memory (loosely coupled)
  * benefits include: more throughput, reliability, can share peripherals
  * drawbacks include: more complicated design 
* **1.10**
  * most important part is ensureing that you meet the deadline

### Ch2
1. **Prefetching:** overlapping IO with ones own computation (controlled by user)
  * **Spooling:** overlapping IO with computation for other jobs (controlled by system)
2.  two modes creates the concept of **priveleged instructions**
  * user mode users can't execute these types of high-rish, priveleged instructions
3. **Trap** is generated by software (system calls). **Interrupt** is generated by hardware.
4. **DMA** is Direct Memory Access
  * useful for transferring large amounts of data between devices
  * holds the interrupts til the end of a block of memory, instead of after each byte
5. all should be restricted except "read the clock"
7. **software** can be used to implement this protection
8. protected: change to monitor mode, write into monitor memory, turn off timer
9. **caches** are useful because they provide faster access than main memory
  * problems they can cause: cache incoherence (outdated/bad data)
10. dual mode (user/monitor), memory protection, CPU protection
  * _two registers_ for memory protection: **base and limit**
  * to protect the CPU: timer that allows for a context switch to happen

### Ch3
1. Five major activities in **Process Management**: Creating/Deleting, Suspend, Synchronization, Communication, Deadlock
2. Three main activities in **Memory Management**: 
  * need to keep track of which part of memory being used, and by whom
  * which processes should be loaded into free memory
  * release space once processes have been terminated
3. 3 things for secondary storage: free space management, allocating free space to a file, disk scheduling
4. 5 activities for file management
   * creation/deletion of file, creation/deletion of dir, renaming?, map files onto disk, backup files
5. what purpose is command interpreter?
  * interprets commands from user and makes appropriate system calls
  * seperate from kernal, located in the user space
    * this allows user to make many changes
6. five services provided by OS: Program Execution (allocating all the resources, etc), IO Operations (these are privleged instructions), File System (helps manage the large number of files), Communication System, Error Detection (some user errors can be caught by the system)
7. **System Calls** allow user to request a service from the OS
10. **System Programs** are a set of system calls
  * for example, the file system 
11. **layered approach** is good because it is easier to debug (layer by layer)
  * can have a slower _response time_
12. **microkernel approach** uses a small kernel so its more convenient to make changes (to user space)
  * small kernel is also easier to debug
13. using a **virtual machine** is more secure becuase it seperates user spaces
  * allows easier maintenence done since systems are seperate
  * user can run programs that weren't designed for their machine (different platforms)
16. synthesis vs. layered -> they're total opposites

### Ch 4
1. dos/concurrent processing:
  * allows multiple processes to run at the same time
    * problems include: synchronization, scheduling, deadlocks
2. short-term vs long/medium-term:
  * short-term: AKA the CPU scheduler
    * uses a 'ready' queue
  * long-term scheduler: job pool located on disk.. some jobs need to be brought into main memory
    * some partially-executed jobs get 'swapped-out' back onto the disk
      * this happens when system is saturated (too busy)
      * ^^ done by the 'medium-term' scheduler
3. this question about context switch...
  * if new context is already loaded into registers, we just get it from there..
  * if new context is in memory and registers are in use... we save a register set into memory so it can be used
4. first, save current context (p0) into main memory (pcb0)
  * then load context of p1 from memory (pcb1)

### Ch 6
1. `n!` (n - factorial) ways to order `n` processes 
2. preemptive means a new process can take the CPU away, non-preemptive means new processes cannot take the CPU away
  * non-preemptive wouldn't be used in a lab/computer center becuase someone could tie up all the machines forever
3. **gantt charts** <- be able to work through these burst time/priority examples
  * turnaround time (equals burst time + wait time)
  * waiting time
  * finding minimum average waiting time (always Shortest Job First)
  * **future-knowledge** scheduling allows us to prevent issues where a long job starts before some short ones come in
5. if you had two pointers to the same process in the ready queue, you would get two 'quantums' worth of time per cycle
  * an advantage would be that you'd give more time to a particular process, disadvantage would be how this would affect other processes
  * this could be achieved by doubling the 'time quantum'
6. by using different time-quantums you can get quick results from some processes... if you take it all the way you get **FCFS**
7. B > a > 0 = **FCFS**
8. a < B < 0 = **LIFO**
9. A) SJF _is_ priority scheduling, where the priority is based on which job is shortest
  * B) The lowest level of Multilevel feedback queue _is_ FCFS
  * C) if priority is defined as 'arrival time' then you get FCFS
  * D) in both RR and SJF, the shortest job will be finished first